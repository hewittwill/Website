## Collaborative Data Science

One of the most common problems for a data science team is working together efficiently. Data science (this notion applying to scientific computing in general) differs substantially from regular software engineering as the problem is generally not how to collaborate on a large volume of code, but on high-density and high-complexity code. 

There are a few other problems when collaborating on data science projects when compared to regular software engineering, outputs are not always deterministic (i.e. there is a random component to the model, meaning if you ran the same experiment twice you would expect two different results). The artifacts of data science projects (trained model files) are generally massive, take a long time to generate (it's not atypical to train a model for days at a time) and it is hard to measure whenever the change you implemented actually had the desired effect (i.e. did your slightly modified hyperparameter improve accuracy?). This is opposed to regular software engineering where your artifact is usually a small binary, can be generated quickly, and you can easily see if your change had the desired effect (did the new feature you added work).

One final problem about working in a data science team (I move on to how we solve this at HeartLab in the next paragraph!) is useful and available compute and storage resources. It doesn't really make sense for everyone to have their own workstation with so much storage and compute that they can store (the usually massive datasets) we encounter regularly in data science as well as sufficient compute resource to analyse it. Having shared infrastructure for this has the added benefit that individual team members can use more portable and attractive laptops and just remote in to use compute infrastructure. 

This is a problem I'd grappled with throughout my data science career. First when I was just learning to code for data science - in this situation everything was on my laptop or AWS anyway and I didn't have to worry about Git repos or sharing results much. You also build a degree of instinct for data science projects, that as a team grows it becomes harder to share. That instict means you can generally tell what hyperparameters you used to train a model just by looking at training graphs and model outputs. 

But then as I moved on to working in a research institute, and now into HeartLab where we have been growing our data science team, it has become a substantially more difficult problem to solve. As we a) grew and b) I got too busy to keep very regular updates (which has the added downside of leaning towards micromanagement) on what the team was working on, I struggled to maintain a sense of the direction and progress of the research team. This lead to a degree of organised chaos - there was great innovation and people were churning out great ideas, but because of the poor visibility across the team there was a lot of overlapping code, datasets stored in an unorganized format (meaning we ended up with multiple versions of the same dataset) and model outputs were stored in random places. 

I went through many different iterations of how to solve this problem - trially both weekly and daily research updates. Though improving visibility, they reduced productive time and meant we were wasting high-IQ time on trying to explain complex concepts to eachother, a task usually performed by commit messages in other software engineering projects. We also went through a phase of strict "everything has to be in a git repo, and if you work on a bit of code you have to push it at the end of the day." This just meant everyone wrote rushed commit messages to meet the commit deadline, and also was pushing junk code to the git repo - not junk in it wasn't useful, but junk in it wasn't useful broadly.

This issue became even more salient during the COVID-19 lockdown, as now we didn't have the high-bandwith communication channel of neighbouring desks. I began searching for solutions to this problem and came across [this](https://expel.io/blog/our-journey-jupyterhub-beyond/) article by Peter Silberman, CTO at Expel explaining their solution to these problems. Expel followed a broadly similar pathway to us - struggling with data sprawl and visibility until they came across this solution called [JupyterHub](https://jupyter.org/hub).

Jupyter Notebooks in their raw form are great for a learning tool - you can embed rich-text and markdown alongside cells right inside code files called 'notebooks' and can execute cells seperately. Although my personal view is that they should not replace the important learning curve of having to learn to code in nothing but a bash terminal, where all you have is `nano` (I never learnt vim or emacs) and a command line. Unfortunately though they can be cumbersome to share around.