## Collaborative Data Science

One of the most common problems for a data science team is working together efficiently. Data science (this notion applying to scientific computing in general) differs substantially from regular software engineering as the problem is generally not how to collaborate on a large volume of code, but on high-density and high-complexity code. 

There are a few other problems when collaborating on data science projects when compared to regular software engineering, outputs are not always deterministic (i.e. there is a random component to the model, meaning if you ran the same experiment twice you would expect two different results). The artifacts of data science projects (trained model files) are generally massive, take a long time to generate (it's not atypical to train a model for days at a time) and it is hard to measure whenever the change you implemented actually had the desired effect (i.e. did your slightly modified hyperparameter improve accuracy?). This is opposed to regular software engineering where your artifact is usually a small binary, can be generated quickly, and you can easily see if your change had the desired effect (did the new feature you added work).

One final problem about working in a data science team (I move on to how we solve this at HeartLab in the next paragraph!) is useful and available compute and storage resources. It doesn't really make sense for everyone to have their own workstation with so much storage and compute that they can store (the usually massive datasets) we encounter regularly in data science as well as sufficient compute resource to analyse it. Having shared infrastructure for this has the added benefit that individual team members can use more portable and attractive laptops and just remote in to use compute infrastructure. 

This is a problem I'd grappled with throughout my data science career. First when I was just learning to code for data science - in this situation everything was on my laptop or AWS anyway and I didn't have to worry about Git repos or sharing results much. You also build a degree of instinct for data science projects, that as a team grows it becomes harder to share. That instict means you can generally tell what hyperparameters you used to train a model just by looking at training graphs and model outputs. 

But then as I moved on to working in a research institute, and now into HeartLab where we have been growing our data science team, it has become a substantially more difficult problem to solve. As we a) grew and b) I got too busy to keep very regular updates (which has the added downside of leaning towards micromanagement) on what the team was working on, I struggled to maintain a sense of the direction and progress of the research team. This lead to a degree of organised chaos - there was great innovation and people were churning out great ideas, but because of the poor visibility across the team there was a lot of overlapping code, datasets stored in an unorganized format (meaning we ended up with multiple versions of the same dataset) and model outputs were stored in random places. 

I went through many different iterations of how to solve this problem - trially both weekly and daily research updates. Though improving visibility, they reduced productive time and meant we were wasting high-IQ time on trying to explain complex concepts to eachother, a task usually performed by commit messages in other software engineering projects. We also went through a phase of strict "everything has to be in a git repo, and if you work on a bit of code you have to push it at the end of the day." This just meant everyone wrote rushed commit messages to meet the commit deadline, and also was pushing junk code to the git repo - not junk in it wasn't useful, but junk in it wasn't useful broadly.

This issue became even more salient during the COVID-19 lockdown, as now we didn't have the high-bandwith communication channel of neighbouring desks. I began searching for solutions to this problem and came across [this](https://expel.io/blog/our-journey-jupyterhub-beyond/) article by Peter Silberman, CTO at Expel explaining their solution to these problems. Expel followed a broadly similar pathway to us - struggling with data sprawl and visibility until they came across this solution called [JupyterHub](https://jupyter.org/hub).

Jupyter Notebooks in their raw form are great for a learning tool - you can embed rich-text and markdown alongside cells right inside code files called 'notebooks' and can execute cells seperately. Although my personal view is that they should not replace the important learning curve of having to learn to code in nothing but a bash terminal, where all you have is `nano` (I never learnt vim or emacs) and a command line. Unfortunately though they can be painful to share around - you can't just open one in any old text editor so you have to spin up some sort of Jupyter server. Plus they generate those annoying checkpoint files - which are great for when you're actually focusing on a bit of code, but don't share easily.

Enter JupyterHub - an edition of the normal single-user Jupyter server that adds authentication and a workload management system (which can also run on a container orchestration system!). Run JupyterHub on your HPC cluster, link it to your organisations OAuth system and boom - the convenience of Jupyter notebooks combined with the power of accessing your entire HPC cluster from one simple web portal.  

I'm going to avoid details about the technical detail about how we set up our JupyterHub environment, and focus more on the actual innovation culture it enhances. But briefly - our JupyterHub environment links everyone by default to a "Welcome" notebook, that re-iterates crucial rules of the environment and convenient links. Also reminders on how to add conda environments etc. Once you head to your file browser everyones linked to a shared folder, which holds everyones home folder as well as a shared dataset folder. Everybodies home folder is like their desk at work - personalise it how they will. But if a notebook becomes substantially important to the rest of the team then your meant to tidy it up, port it to what we label a "mature project" (read: actually document it properly) and put into a git repo. Everyone has read+execute permissions on everyones home folders, but only write in their own home folder. Team visibility = achieved. 

The dataset folder is the lowtech solution to dataset versioning - which there really isn't an elegant solution to yet. In the top level directory is a README notebook that sets out a) rules on how to use the dataset folder and b) blurbs and technical definitions for each dataset version. Once you've done some preprocessing and cut a new version of the dataset the folder becomes read-only to make sure we can repeat experiments. Not great for storage efficiency but probably better than the dataset sprawl that existed before.

Aside from the actual technical advantages of the platform, the innovation principals it underlies are transparency, communication and scientific reproducability. 